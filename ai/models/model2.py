import os
import random
import numpy as np
import tensorflow as tf
from tf_agents.environments import py_environment, tf_py_environment, utils
from tf_agents.specs import array_spec
from tf_agents.trajectories import time_step as ts
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent
from tf_agents.utils import common
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory, policy_step
import json

# ì§€ë„í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (í•„ìš”ì‹œ ì‚¬ìš©)
from tensorflow.keras import models
inference_model = models.load_model(os.path.join("ai", "models", "inference_model.h5"), compile=False)

# ë°ì´í„°ì…‹ ë¡œë“œ
with open(os.path.join("ai", "dataset", "final_dataset_with_family_and_size.json"), 'r', encoding='utf-8') as f:
    data = json.load(f)
    animals = data["animals"]
    unique_animals = [a["name"] for a in animals]

print("Unique animals:", unique_animals)

# -------------------------------
# í™˜ê²½ ì •ì˜ (TwentyQuestionsTFEnv)
# -------------------------------
class TwentyQuestionsTFEnv(py_environment.PyEnvironment):
    def __init__(self, dataset):
        self.dataset = dataset
        self.num_questions = len(dataset[0]['questions'])
        self.unique_animals = [a["name"] for a in dataset]
        self.num_animals = len(self.unique_animals)

        self.total_actions = self.num_questions + self.num_animals  # ì§ˆë¬¸ + ì¶”ì¸¡

        self._action_spec = array_spec.BoundedArraySpec(
            shape=(), dtype=np.int32, minimum=0, maximum=self.total_actions - 1, name='action')

        self._observation_spec = array_spec.BoundedArraySpec(
            shape=(self.num_questions,), dtype=np.float32, minimum=-1, maximum=2, name='observation')

        self._episode_ended = False
        self.reset()

    def action_spec(self):
        return self._action_spec

    def observation_spec(self):
        return self._observation_spec

    def _reset(self):
        self.target = random.choice(self.dataset)
        self.asked_questions = set()
        self.history = [-1] * self.num_questions
        self._episode_ended = False
        return ts.restart(np.array(self.history, dtype=np.float32))

    def _step(self, action):
        action = int(np.squeeze(action))
        if self._episode_ended:
            return self.reset()

        if action < self.num_questions:
            # ì§ˆë¬¸ í–‰ë™
            if action in self.asked_questions:
                reward = -0.5
            else:
                self.asked_questions.add(action)
                answer = self.target['questions'][action]['answer']
                self.history[action] = answer
                reward = -0.1
            return ts.transition(np.array(self.history, dtype=np.float32), reward=reward, discount=1.0)

        else:
            # ì¶”ì¸¡ í–‰ë™
            guess_index = action - self.num_questions
            guess_name = self.unique_animals[guess_index]
            self._episode_ended = True
            reward = 100.0 if guess_name == self.target['name'] else -50.0
            return ts.termination(np.array(self.history, dtype=np.float32), reward=reward)



# -------------------------------
# í™˜ê²½ ë° ì—ì´ì „íŠ¸ êµ¬ì„± (ê°•í™”í•™ìŠµ ë¶€ë¶„)
# -------------------------------
train_py_env = TwentyQuestionsTFEnv(animals)
train_env = tf_py_environment.TFPyEnvironment(train_py_env)

fc_layer_params = (100,)
q_net = q_network.QNetwork(
    train_env.observation_spec(),
    train_env.action_spec(),  # ìë™ìœ¼ë¡œ total_actions ì ìš©ë¨
    fc_layer_params=(100,)
)


# ì‚¬ìš©: ê²½ì‚¬ í´ë¦¬í•‘(clipnorm=1.0)ê³¼ í•™ìŠµë¥  ì¡°ì •
optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)
train_step_counter = tf.Variable(0)

# Îµ-greedy íƒìƒ‰ ì •ì±…ì„ ìœ„í•œ ì´ˆê¸° Îµ ê°’ê³¼ ê°ì†Œìœ¨
initial_epsilon = 1.0
final_epsilon = 0.1
epsilon_decay_steps = 10000

agent = dqn_agent.DqnAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    td_errors_loss_fn=common.element_wise_huber_loss,
    train_step_counter=train_step_counter,
    epsilon_greedy=lambda: np.interp(
        train_step_counter.numpy(), [0, epsilon_decay_steps], [initial_epsilon, final_epsilon]
    )
)
agent.initialize()

replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=train_env.batch_size,
    max_length=10000
)

from tf_agents.policies import random_tf_policy
random_policy = random_tf_policy.RandomTFPolicy(
    train_env.time_step_spec(), train_env.action_spec()
)

def collect_step(environment, policy, buffer):
    time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = trajectory.from_transition(time_step, action_step, next_time_step)
    buffer.add_batch(traj)

# ì´ˆê¸° ê²½í—˜ ìˆ˜ì§‘ (ìë™ìœ¼ë¡œ 100ë²ˆ ì§„í–‰)
for _ in range(100):
    collect_step(train_env, random_policy, replay_buffer)

dataset = replay_buffer.as_dataset(sample_batch_size=64, num_steps=2).prefetch(3)
iterator = iter(dataset)

checkpoint_dir = "ai/checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)
checkpoint = tf.train.Checkpoint(
    agent=agent, optimizer=optimizer, train_step_counter=train_step_counter
)
latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)
if latest_ckpt:
    checkpoint.restore(latest_ckpt)
    print(f"Checkpoint {latest_ckpt} ë³µì›ë¨.")


# -------------------------------
# ìë™í™” ì—í”¼ì†Œë“œ ìƒì„± (simulate_episode)
# -------------------------------
def simulate_episode(env, target_animal):
    """
    ì£¼ì–´ì§„ target_animal ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ë‹µ(ì‚¬ìš©ì ì •ë‹µì²˜ëŸ¼)ì„ ì…ë ¥í•˜ëŠ” ì—í”¼ì†Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìë™ í•™ìŠµì„ ìœ„í•œ ê²½í—˜ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    env.target = target_animal
    env.asked_questions = set()
    correct_answers = [q['answer'] for q in target_animal['questions']]
    env.history = correct_answers.copy()
    env.asked_questions = set(range(env.num_questions))
    env._episode_ended = True
    final_time_step = env.guess(target_animal['name'])
    batched_state = np.array([env.history], dtype=np.float32)
    user_time_step = ts.TimeStep(
        step_type=np.array([ts.StepType.FIRST], dtype=np.int32),
        reward=np.array([0.0], dtype=np.float32),
        discount=np.array([1.0], dtype=np.float32),
        observation=batched_state
    )
    user_next_time_step = ts.TimeStep(
        step_type=np.array([ts.StepType.LAST], dtype=np.int32),
        reward=np.array([float(final_time_step.reward)], dtype=np.float32),
        discount=np.array([0.0], dtype=np.float32),
        observation=batched_state
    )
    dummy_action = np.array([0], dtype=np.int32)
    dummy_action_step = policy_step.PolicyStep(action=dummy_action, state=())
    user_traj = trajectory.from_transition(
        user_time_step, dummy_action_step, user_next_time_step
    )
    return user_traj, final_time_step


# -------------------------------
# ìë™ í•™ìŠµ ë£¨í”„: ì‹œë®¬ë ˆì´ì…˜ ì—í”¼ì†Œë“œë¡œ í•™ìŠµí•˜ê¸°
# -------------------------------
def simulate_episode_with_guess(env, policy, buffer):
    """
    ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì •ë‹µ ì¶”ì¸¡ê¹Œì§€ ìŠ¤ìŠ¤ë¡œ ì§„í–‰í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ ì—í”¼ì†Œë“œ.
    ì—í”¼ì†Œë“œ í•˜ë‚˜ì˜ trajectoryë¥¼ replay bufferì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    time_step = env.reset()
    episode_length = 0

    while not time_step.is_last():
        action_step = policy.action(time_step)
        next_time_step = env.step(action_step.action)

        traj = trajectory.from_transition(time_step, action_step, next_time_step)
        buffer.add_batch(traj)

        time_step = next_time_step
        episode_length += 1

    return episode_length


def automated_training(num_episodes=1000, steps_per_episode=100):
    for ep in range(num_episodes):
        episode_length = simulate_episode_with_guess(train_env, agent.collect_policy, replay_buffer)

        for _ in range(steps_per_episode):
            experience, _ = next(iterator)
            loss_info = agent.train(experience)

        if (ep + 1) % 100 == 0:
            ckpt_path = checkpoint.save(os.path.join(checkpoint_dir, "ckpt"))
            print(f"[ì—í”¼ì†Œë“œ {ep+1}] ì—í”¼ì†Œë“œ ê¸¸ì´: {episode_length}, ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {ckpt_path}. Loss: {loss_info.loss.numpy()}")

    print("ğŸ‰ ìë™ í•™ìŠµ ì™„ë£Œ!")



# -------------------------------
# ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í†µí•© ì¶”ë¡  í•¨ìˆ˜ (ì„ íƒ ì‚¬í•­)
# -------------------------------
def integrated_inference():
    """
    ì‚¬ìš©ìê°€ ìƒê°í•œ ë™ë¬¼ì„ ë§ì¶”ê¸° ìœ„í•´, AIëŠ” ì§ˆë¬¸ì„ í†µí•´ í›„ë³´ ëª©ë¡ì„ ì¢í™ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°›ì•„ í›„ë³´ ëª©ë¡ì„ ì¶œë ¥í•˜ê³ , ìµœì¢… ì¶”ì¸¡ í›„ ê²½í—˜ì„ ì €ì¥ ë° ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    print("ì‚¬ìš©ìê°€ ìƒê°í•˜ëŠ” ë™ë¬¼ì„ ë§ì¶”ê² ìŠµë‹ˆë‹¤.")
    correct_animal = input("ì‚¬ìš©ìê°€ ìƒê°í•œ ë™ë¬¼(ì •ë‹µ)ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    print("----- ì§ˆë¬¸ì„ ì‹œì‘í•©ë‹ˆë‹¤. -----")
    state = train_env.reset()
    asked = set()
    guess_animal = None

    while True:
        available = set(range(train_py_env.num_questions)) - asked
        if not available:
            print("ëª¨ë“  ì§ˆë¬¸ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤.")
            break

        action_step = agent.policy.action(state)
        q_index = int(np.squeeze(action_step.action))
        if q_index not in available:
            q_index = random.choice(list(available))
        asked.add(q_index)

        question_text = train_py_env.dataset[0]['questions'][q_index]['question']
        print(f"ì§ˆë¬¸ {q_index+1}: {question_text}")
        try:
            user_input = input("ë‹¹ì‹ ì˜ ë‹µë³€ (1=ì˜ˆ, 0=ì•„ë‹ˆì˜¤, 2=ëª¨ë¥´ê² ë‹¤): ").strip()
            user_answer = int(user_input)
            if user_answer not in [0, 1, 2]:
                print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. -1 (ë¯¸ë‹µë³€)ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                user_answer = -1
        except Exception:
            print("ì…ë ¥ ì˜¤ë¥˜ ë°œìƒ, -1 (ë¯¸ë‹µë³€)ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            user_answer = -1

        train_py_env.history[q_index] = user_answer
        train_py_env.asked_questions.add(q_index)
        state = ts.transition(np.array([train_py_env.history], dtype=np.float32),
                              reward=-0.1,
                              discount=1.0)

        answered_count = sum(1 for x in train_py_env.history if x != -1)
        if answered_count >= 5:
            scores = {}
            for animal in train_py_env.dataset:
                score = 0
                for i, ans in enumerate(train_py_env.history):
                    if ans == -1:
                        continue
                    if ans == animal['questions'][i]['answer']:
                        score += 1
                scores[animal['name']] = score
            max_score = max(scores.values())
            candidates = [name for name, score in scores.items() if score == max_score]
            confidence = max_score / answered_count if answered_count > 0 else 0

            print("----------------------------")
            print("í˜„ì¬ í›„ë³´ ë™ë¬¼:", candidates)
            print("í˜„ì¬ ë‹µë³€ ë²¡í„°:", train_py_env.history)
            print("----------------------------")
            if len(candidates) == 1 and confidence >= 0.9:
                print("í›„ë³´ ëª©ë¡ì´ 1ê°œë¡œ ì¢í˜€ì¡ŒìŠµë‹ˆë‹¤.")
                guess_animal = candidates[0]
                break
            elif confidence >= 0.9 and len(candidates) <= 3:
                for idx, candidate in enumerate(candidates):
                    print(f"{idx+1}. {candidate}")
                choice = input("ì´ ì¤‘ ì˜¬ë°”ë¥¸ ë™ë¬¼ì„ ì„ íƒí•˜ì„¸ìš” (ë²ˆí˜¸ ì…ë ¥, ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ê³„ì† ì§ˆë¬¸í•©ë‹ˆë‹¤): ").strip()
                if choice:
                    try:
                        choice = int(choice)
                        if 1 <= choice <= len(candidates):
                            guess_animal = candidates[choice - 1]
                            break
                        else:
                            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê³„ì† ì§ˆë¬¸ ì§„í–‰.")
                    except Exception:
                        print("ì…ë ¥ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ê³„ì† ì§ˆë¬¸ ì§„í–‰.")
    print("----- ì§ˆë¬¸ ì¢…ë£Œ -----")
    print("ì…ë ¥í•œ ë‹µë³€ ë²¡í„°:", train_py_env.history)
    
    final_guess = input("ìµœì¢… ì¶”ì¸¡í•  ë™ë¬¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: í›„ë³´ ë˜ëŠ” ì§€ë„í•™ìŠµ ëª¨ë¸ ì‚¬ìš©): ").strip()
    if not final_guess:
        if guess_animal is not None:
            final_guess = guess_animal
        else:
            inference_input = np.array(train_py_env.history, dtype=np.float32).reshape(1, -1)
            pred_probs = inference_model.predict(inference_input)
            pred_class = np.argmax(pred_probs, axis=1)[0]
            final_guess = unique_animals[pred_class]
            print("ì§€ë„í•™ìŠµ ëª¨ë¸ ìµœì¢… ì˜ˆì¸¡ ë™ë¬¼:", final_guess)
    
    final_time_step = train_py_env.guess(final_guess)
    print("ì¶”ì¸¡ ê²°ê³¼ ë³´ìƒ:", float(final_time_step.reward))
    print("ì‹¤ì œ ì •ë‹µ ë™ë¬¼:", correct_animal)
    
    batched_final_state = np.array([train_py_env.history], dtype=np.float32)
    user_time_step = ts.TimeStep(
        step_type=np.array([ts.StepType.FIRST], dtype=np.int32),
        reward=np.array([0.0], dtype=np.float32),
        discount=np.array([1.0], dtype=np.float32),
        observation=batched_final_state
    )
    user_next_time_step = ts.TimeStep(
        step_type=np.array([ts.StepType.LAST], dtype=np.int32),
        reward=np.array([float(final_time_step.reward)], dtype=np.float32),
        discount=np.array([0.0], dtype=np.float32),
        observation=batched_final_state
    )
    dummy_action = np.array([0], dtype=np.int32)
    dummy_action_step = policy_step.PolicyStep(action=dummy_action, state=())
    user_traj = trajectory.from_transition(user_time_step, dummy_action_step, user_next_time_step)
    replay_buffer.add_batch(user_traj)
    print("ì‚¬ìš©ì ê²½í—˜ì´ Replay Bufferì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ì¶”ê°€ ê²½í—˜ì„ ì´ìš©í•˜ì—¬ 100 ìŠ¤í…ì˜ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤...")
    for i in range(100):
        experience, _ = next(iterator)
        loss_info = agent.train(experience)
        if (i + 1) % 100 == 0:
            ckpt_path = checkpoint.save(os.path.join(checkpoint_dir, "ckpt"))
            print(f"Step {agent.train_step_counter.numpy()} checkpoint saved at {ckpt_path}. Loss: {loss_info.loss.numpy()}")
    print("ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!")


# -------------------------------
# ì‹¤í–‰ ì„ íƒ
# -------------------------------
if __name__ == "__main__":
    mode = input("ìë™ í•™ìŠµì„ ì§„í–‰í•˜ë ¤ë©´ 'auto'ë¥¼, ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìœ¼ë ¤ë©´ ê·¸ëƒ¥ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”: ").strip().lower()
    if mode == "auto":
        print("ìë™ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        automated_training(num_episodes=1000, steps_per_episode=100)
    else:
        integrated_inference()
